{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68P1BfhPb_5o"
   },
   "source": [
    "# Final Exam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5P4etsd7cLON"
   },
   "source": [
    "## 1.  From the perspective of a social scientist, which models did we learn this semester that are useful for ruling out alternative explanations through control variables AND that allow us to observe substantively meaningful information from model coefficients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KskApODiQF9B"
   },
   "source": [
    "In my opinion, both linear regression and logistic regression model we learned this semester that are useful for ruling out alternative explanations through control variables and that allow us to observe substantively meaningful information from model coefficients. The reason is that linear regression model is a great for know the relationship between a continuous dependent variable and independent variables, and coefficients can be directly interpreted by the results, we can see change in one unit in independent variable will results in how many units change in the dependent varable, in average, and holding all other variables constant. For logistic regression model, it is suitable for binary outcomes and have all the characteristics that the linear regression have. Becauses of the log nature, it will give the percentage change instead of the unit change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nC6yQI-icNnb"
   },
   "source": [
    "## 2. Describe the main differences between supervised and unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWBQBcf0We_A"
   },
   "source": [
    "* The main difference between supervised and unsuperivised learning is work different datam supervised learning works with label data since upervised learning observes both a set of features X1, X2, . . . , Xp for each object, as well as a response or outcome variable Y. Compared to supervised learning, unsupervised learning only observes the features X1, X2, . . . , Xp.\n",
    "* The goal for using supervised learning is to predict an output (Y) based on input features (X), whereas unsupervised learning is much more subjective and the goal for apply unsupervised learning is to get more infomation about the data.\n",
    "* There also have differences on evaluation. For supervised learning, the performance of a supervised learning such as logistic regression can be evaluated based on accuracy of traning set X y, and test set X y. But because of unsupervised learning is much more subjective, the performance of a unsupervised learning is based on how well if the model can identify patterns which meet the requirements of the specific task such as when we do an unsupervised learning on a dog figured picture.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_deo7qdicSsL"
   },
   "source": [
    "## 3. Is supervised or unsupervised learning the primary approach that is used by machine learning practitioners?  For whatever approach you think is secondary, why would you use this approach (what's a good reason to use these kinds of models?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfvaisAIQmnu"
   },
   "source": [
    "Supervised learning is the primary approach that used by machine learning practitioners. One of the reason is that supervised learning observes both a set of features X1, X2, . . . , Xp for each object, as well as a response or outcome variable Y, which means that supervised learning can give clear objective and direct feedback for the prediction. Hence, unsupervised learning is the secondary approach. One good reason to use unsupervised learning is that when we do not have an associated response variable Y in the data, because unsupervised learning only observes the features X1, X2, . . . , Xp. Unsupervised learning is also more subjective, since there is no simple goal for the analysis, which can be applied in many fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Tq_H1qecYvq"
   },
   "source": [
    "## 4. Which unsupervised learning modeling approaches did we cover this semester?  What are the major differences between these techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuMZc_ZcvltE"
   },
   "source": [
    "1. Principal Components Analysis (PCA).\n",
    "2. K-means Clustering.\n",
    "3. Hierarchical Clustering.\n",
    "4. Manifold learning.\n",
    "5. Neural Network.\n",
    "6. Convolutional Neural Network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmcZkE2ley0d"
   },
   "source": [
    "For both Neural Network and Covolutional NN, these modeling approaches can be use in both supervised learning and unsupervised learning. But we mainly did unsupervised learning for NN and CNN so I wrote down in the answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SILF7CpPfW4_"
   },
   "source": [
    "There are a few major differences between these techniques. PCA is a linear method, while manifold learning method we learned are not linear. This difference means that models that are non-linear like manifold learning method are tend to easier to handle more complex data. Another major differences between these techniques is that K-means, hierarchical clustering are focusing on grouping similar data points, while PCA, manifold learning, and neural networks are more focusing on transformation. Convolutional neural network are more focused on data like images while other methods are more genreal. In one sentance, the main difference is that their approach to compare and analyze patterns and reduce its complexity and dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpOyzGvLccZ2"
   },
   "source": [
    "## 5.  What are the main benefits of using Principal Components Analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSk6_S6Yq4_n"
   },
   "source": [
    "The followings are the few of the main benefits of using PCA\n",
    "1. Reduce the complexity of data.\n",
    "* PCA can reduce the complexity of data by reduce the numbers of variables or features from data while remain most of the cricial infomation.\n",
    "2. Speed up machine learning computing processes and algorithms.\n",
    "* Since PCA cna reduce numbers of features, it significantlly decrease the ML processing time. Especially when we deal with a large dataset, PCA can help speed up the process.\n",
    "3. Prevent predictive algorithms from data overfitting issues.\n",
    "* Overfitting issues usually happen when a ML model also learns the noise instead of the underlying pattern. PCA can prevent predictive algorithms from data overfitting issues by less significant features as 1. showed.\n",
    "4. Increase the performance of ML algorithms by eliminating unnecessary correlated variables.\n",
    "* Since PCA transforms the original correlated features into a new set of uncorrelated principal components, thus, PCA increases the proformance of ML algorithms.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoiU66K2cfir"
   },
   "source": [
    "## 6. Thinking about neural networks, what are three major differences between a deep multilayer perceptron network and a convolutional neural network model?  Be sure to define any key terms in your explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skDP94CwYiFA"
   },
   "source": [
    "From what I learned, I think different layers, feature detection, and parameter efficiency are the three major differences between a deep multilayer perceptron network and a convolutional neural network model. First of all, multilayer perceptron network have fully connected layers while CNN only have convolutional layers which are spatially herarchal, which means that CNN is more good at analyzing images and videos. The second is feature detection, CNN can easily detect spatial hierarchies in data as we discussed in first difference, and MLP do not have this function. The last one is parameter efficiency, CNN models are much more efficient because the shared weights in convolutional layers. Although seems like CNN are better than MLP models, MLP models may perform bettern than CNN due to the nature of the data. The reason is that MLP is more suitable to difference types of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzpgdVa_cjfr"
   },
   "source": [
    "## 7. Write the tf.keras code for a multilayer perceptron neural network with the following structure: Three hidden layers.  50 hidden units in the first hidden layer, 100 in the second, and 150 in the third.  Activate all hidden layers with relu.  The output layer should be built to classify to five categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Lw11KIDoddsh"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_dim=100)) # For input_dim, since we do not have real data, I just pul 100 for the place holder.\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sXdVZesfSWx"
   },
   "source": [
    "## 8. Write the tf.keras code for a multilayer perceptron neural network with the following structure: Two hidden layers.  75 hidden units in the first hidden layer and 150 in the second.  Activate all hidden layers with relu.  The output layer should be built to classify a binary dependent variable.  Further, your optimization technique should be stochastic gradient descent. (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dKYwgES9fZgB"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(75, activation='relu', input_dim=100)) # # For input_dim, since we do not have real data, I just pul 100 for the place holder.\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB1y277FjG-9"
   },
   "source": [
    "## 9.  Write the tf.keras code for a convolutional neural network with the following structure: Two convolutional layers.  16 filters in the first layer and 28 in the second.  Activate all convolutional layers with relu.  Use max pooling after each convolutional layer with a 2 by 2 filter.  The output layer should be built to classify to ten categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gSAbuS49jK_a"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, (3, 3), activation='relu',\n",
    "                 input_shape=(28,28,1))) # For input_shape, since we do not have real data, I just pul (28,28,1) for the place holder.\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(28, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kf35-mf4m0QI"
   },
   "source": [
    "## 10. Write the keras code for a convolutional neural network with the following structure: Two convolutional layers.  32 filters in the first layer and 32 in the second.  Activate all convolutional layers with relu.  Use max pooling after each convolutional layer with a 2 by 2 filter.  Add two fully connected layers with 128 hidden units in each layer and relu activations.  The output layer should be built to classify to six categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cFOO1jTWm47H"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, BatchNormalization, Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.python.keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "import tensorflow as tf\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu', input_shape=[192, 192, 3])) # For input_shape, since we do not have real data, I just pul (192, 192, 3) for the place holder.\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
